# Project Workflow: Understanding the System Architecture

## ğŸ¯ Project Overview

This project is a **Cross-Lingual Question Answering (CLQA) Research Platform** that:
- Compares two AI models: **mBERT** (extractive) vs **mT5** (generative)
- Supports **zero-shot learning** (train on English, test on other languages)
- Supports **few-shot learning** (fine-tune with few examples)
- Works with **54 language pairs** (6 question languages Ã— 9 context languages)

---

## ğŸ“ System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CROSS-LINGUAL QA SYSTEM                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚         DATA LAYER                      â”‚
        â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
        â”‚  â”‚  SQuAD   â”‚  â”‚  XQuAD   â”‚  MLQA     â”‚
        â”‚  â”‚  (Train) â”‚  â”‚  (Eval)  â”‚  TyDiQA   â”‚
        â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
        â”‚         â”‚              â”‚               â”‚
        â”‚         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
        â”‚                â–¼                       â”‚
        â”‚      Data Loaders & Preprocessors      â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚         MODEL LAYER                      â”‚
        â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
        â”‚  â”‚    mBERT     â”‚  â”‚     mT5      â”‚   â”‚
        â”‚  â”‚ (Extractive) â”‚  â”‚ (Generative) â”‚   â”‚
        â”‚  â”‚  110M params â”‚  â”‚  580M params â”‚   â”‚
        â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚      TRAINING LAYER                      â”‚
        â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
        â”‚  â”‚ Zero-Shot    â”‚  â”‚  Few-Shot    â”‚   â”‚
        â”‚  â”‚ (English)    â”‚  â”‚ (k examples) â”‚   â”‚
        â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚     EVALUATION LAYER                     â”‚
        â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
        â”‚  â”‚ Metrics  â”‚  â”‚ Compare  â”‚            â”‚
        â”‚  â”‚ (EM/F1)  â”‚  â”‚ Models   â”‚            â”‚
        â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚      INFERENCE LAYER                    â”‚
        â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
        â”‚  â”‚   API    â”‚  â”‚Dashboard â”‚            â”‚
        â”‚  â”‚ (FastAPI)â”‚  â”‚(Streamlit)â”‚          â”‚
        â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”„ Complete Workflow Diagram

```
START
  â”‚
  â”œâ”€â–º [1] SETUP ENVIRONMENT
  â”‚     â”œâ”€â–º Activate venv
  â”‚     â”œâ”€â–º Install dependencies
  â”‚     â””â”€â–º Verify installation
  â”‚
  â”œâ”€â–º [2] DOWNLOAD DATA
  â”‚     â”œâ”€â–º SQuAD (training)
  â”‚     â”œâ”€â–º XQuAD (evaluation)
  â”‚     â””â”€â–º MLQA, TyDiQA (optional)
  â”‚
  â”œâ”€â–º [3] TRAIN MODEL
  â”‚     â”‚
  â”‚     â”œâ”€â–º ZERO-SHOT TRAINING
  â”‚     â”‚     â”œâ”€â–º Load SQuAD (English)
  â”‚     â”‚     â”œâ”€â–º Train mBERT or mT5
  â”‚     â”‚     â”œâ”€â–º Save checkpoint
  â”‚     â”‚     â””â”€â–º Track experiment
  â”‚     â”‚
  â”‚     â””â”€â–º FEW-SHOT TRAINING (optional)
  â”‚           â”œâ”€â–º Load zero-shot checkpoint
  â”‚           â”œâ”€â–º Sample k examples per language
  â”‚           â”œâ”€â–º Fine-tune
  â”‚           â””â”€â–º Save checkpoint
  â”‚
  â”œâ”€â–º [4] EVALUATE MODEL
  â”‚     â”œâ”€â–º Load checkpoint
  â”‚     â”œâ”€â–º Test on dev set
  â”‚     â”œâ”€â–º Calculate metrics (EM, F1, BLEU, ROUGE)
  â”‚     â””â”€â–º Save results
  â”‚
  â”œâ”€â–º [5] CROSS-LINGUAL EVALUATION
  â”‚     â”œâ”€â–º Test on XQuAD (multiple languages)
  â”‚     â”œâ”€â–º Test on MLQA (language pairs)
  â”‚     â””â”€â–º Analyze performance by language
  â”‚
  â”œâ”€â–º [6] COMPARE MODELS
  â”‚     â”œâ”€â–º Load mBERT results
  â”‚     â”œâ”€â–º Load mT5 results
  â”‚     â”œâ”€â–º Statistical analysis
  â”‚     â””â”€â–º Generate comparison report
  â”‚
  â”œâ”€â–º [7] DEPLOY & USE
  â”‚     â”œâ”€â–º Start API server
  â”‚     â”œâ”€â–º Launch dashboard
  â”‚     â””â”€â–º Make predictions
  â”‚
  â””â”€â–º END (Results in experiments/ directory)
```

---

## ğŸ“‚ File Structure & Purpose

```
Bert_VS_T5/
â”‚
â”œâ”€â”€ ğŸ“ src/                          # Source code
â”‚   â”œâ”€â”€ data/                       # Data handling
â”‚   â”‚   â”œâ”€â”€ squad_loader.py         # Load SQuAD dataset
â”‚   â”‚   â”œâ”€â”€ xquad_loader.py         # Load XQuAD dataset
â”‚   â”‚   â”œâ”€â”€ mlqa_loader.py          # Load MLQA dataset
â”‚   â”‚   â””â”€â”€ multilingual_preprocessor.py  # Process multilingual text
â”‚   â”‚
â”‚   â”œâ”€â”€ models/                     # Model implementations
â”‚   â”‚   â”œâ”€â”€ mbert_wrapper.py        # mBERT model wrapper
â”‚   â”‚   â”œâ”€â”€ mt5_wrapper.py         # mT5 model wrapper
â”‚   â”‚   â””â”€â”€ base_model.py          # Base model interface
â”‚   â”‚
â”‚   â”œâ”€â”€ training/                   # Training logic
â”‚   â”‚   â”œâ”€â”€ zero_shot_trainer.py   # Zero-shot training
â”‚   â”‚   â”œâ”€â”€ few_shot_trainer.py    # Few-shot training
â”‚   â”‚   â””â”€â”€ experiment_tracker.py  # Track experiments
â”‚   â”‚
â”‚   â”œâ”€â”€ evaluation/                 # Evaluation tools
â”‚   â”‚   â”œâ”€â”€ evaluator.py           # Main evaluator
â”‚   â”‚   â”œâ”€â”€ metrics.py             # Calculate metrics
â”‚   â”‚   â””â”€â”€ model_comparison.py    # Compare models
â”‚   â”‚
â”‚   â”œâ”€â”€ inference/                  # Inference engine
â”‚   â”‚   â”œâ”€â”€ model_manager.py       # Manage loaded models
â”‚   â”‚   â””â”€â”€ request_handler.py     # Handle prediction requests
â”‚   â”‚
â”‚   â””â”€â”€ api/                        # REST API
â”‚       â””â”€â”€ server.py              # FastAPI server
â”‚
â”œâ”€â”€ ğŸ“ scripts/                     # Executable scripts
â”‚   â”œâ”€â”€ train_zero_shot.py         # Train zero-shot model
â”‚   â”œâ”€â”€ train_few_shot.py          # Train few-shot model
â”‚   â”œâ”€â”€ evaluate.py                # Evaluate model
â”‚   â”œâ”€â”€ compare_models.py          # Compare two models
â”‚   â””â”€â”€ download_data.py           # Download datasets
â”‚
â”œâ”€â”€ ğŸ“ configs/                     # Configuration files
â”‚   â”œâ”€â”€ model/                     # Model configs
â”‚   â”‚   â”œâ”€â”€ mbert.yaml             # mBERT settings
â”‚   â”‚   â””â”€â”€ mt5.yaml               # mT5 settings
â”‚   â”œâ”€â”€ training/                  # Training configs
â”‚   â”‚   â”œâ”€â”€ zero_shot.yaml         # Zero-shot settings
â”‚   â”‚   â””â”€â”€ few_shot.yaml          # Few-shot settings
â”‚   â””â”€â”€ dataset/                    # Dataset configs
â”‚
â”œâ”€â”€ ğŸ“ data/                        # Datasets (downloaded)
â”‚   â”œâ”€â”€ squad/                     # SQuAD 2.0
â”‚   â”œâ”€â”€ xquad/                     # XQuAD
â”‚   â”œâ”€â”€ mlqa/                      # MLQA
â”‚   â””â”€â”€ tydiqa/                    # TyDiQA
â”‚
â”œâ”€â”€ ğŸ“ models/                      # Trained models
â”‚   â”œâ”€â”€ mbert/                     # mBERT checkpoints
â”‚   â””â”€â”€ checkpoints/               # mT5 checkpoints
â”‚
â”œâ”€â”€ ğŸ“ experiments/                 # Results
â”‚   â”œâ”€â”€ tracking/                  # Experiment metadata
â”‚   â””â”€â”€ evaluations/               # Evaluation results
â”‚
â”œâ”€â”€ ğŸ“ notebooks/                   # Jupyter notebooks
â”‚   â”œâ”€â”€ 01_data_exploration.ipynb  # Explore data
â”‚   â”œâ”€â”€ 02_model_training.ipynb    # Training examples
â”‚   â”œâ”€â”€ 03_evaluation_visualization.ipynb  # Visualize results
â”‚   â””â”€â”€ 04_api_usage.ipynb         # API examples
â”‚
â”œâ”€â”€ ğŸ“ logs/                        # Training logs
â”‚
â”œâ”€â”€ app.py                          # Streamlit dashboard
â”œâ”€â”€ requirements.txt                # Python dependencies
â””â”€â”€ README.md                       # Main documentation
```

---

## ğŸ”€ Data Flow

### Training Flow:
```
SQuAD JSON
    â”‚
    â–¼
Data Loader (squad_loader.py)
    â”‚
    â–¼
Preprocessor (multilingual_preprocessor.py)
    â”‚
    â–¼
Model (mbert_wrapper.py or mt5_wrapper.py)
    â”‚
    â–¼
Trainer (zero_shot_trainer.py)
    â”‚
    â–¼
Checkpoint (models/checkpoints/)
    â”‚
    â–¼
Experiment Tracker (experiments/tracking/)
```

### Inference Flow:
```
User Question + Context
    â”‚
    â–¼
API Server (src/api/server.py)
    â”‚
    â–¼
Request Handler (src/inference/request_handler.py)
    â”‚
    â–¼
Model Manager (src/inference/model_manager.py)
    â”‚
    â–¼
Model (mbert_wrapper.py or mt5_wrapper.py)
    â”‚
    â–¼
Answer + Confidence
```

### Evaluation Flow:
```
Dataset (XQuAD, MLQA, etc.)
    â”‚
    â–¼
Data Loader
    â”‚
    â–¼
Evaluator (src/evaluation/evaluator.py)
    â”‚
    â–¼
Metrics Calculator (src/evaluation/metrics.py)
    â”‚
    â–¼
Results JSON (experiments/evaluations/)
```

---

## ğŸ“ Learning Paths

### Path 1: Quick Results (30-45 minutes)
```
Setup â†’ Download Data â†’ Quick Train â†’ Evaluate â†’ Done
```

### Path 2: Full Research (2-4 hours)
```
Setup â†’ Download Data â†’ Full Train â†’ Evaluate â†’ 
Cross-Lingual Eval â†’ Compare Models â†’ Analyze Results
```

### Path 3: Production Use (1-2 hours)
```
Setup â†’ Download Data â†’ Train â†’ Evaluate â†’ 
Start API â†’ Use Dashboard â†’ Make Predictions
```

---

## ğŸ”‘ Key Concepts Explained

### 1. **Zero-Shot Learning**
- **What**: Train model on English data only
- **Why**: Test if model can transfer knowledge to other languages
- **How**: Train on SQuAD (English), test on XQuAD (other languages)
- **Result**: Measures cross-lingual transfer capability

### 2. **Few-Shot Learning**
- **What**: Fine-tune with a few examples (1, 5, 10, or 50) per language
- **Why**: Improve performance on target languages with minimal data
- **How**: Start from zero-shot checkpoint, add few examples, fine-tune
- **Result**: Better performance than zero-shot, less data than full training

### 3. **Extractive vs Generative**
- **mBERT (Extractive)**: Finds answer span directly from context
  - Faster inference
  - Answer must exist in context
  - Example: "Paris" from "Paris is the capital of France"
  
- **mT5 (Generative)**: Generates answer text
  - More flexible
  - Can paraphrase or summarize
  - Example: Can generate "The capital city of France" even if exact phrase not in context

### 4. **Cross-Lingual QA**
- **Same Language**: Question and context in same language (e.g., English-English)
- **Cross-Lingual**: Question and context in different languages (e.g., English-Spanish)
- **Challenge**: Model must understand both languages and transfer knowledge

---

## ğŸ“Š Expected Results Structure

After running experiments, you'll have:

```
experiments/
â”œâ”€â”€ tracking/
â”‚   â”œâ”€â”€ zero_shot_mbert_20251116_181947.json
â”‚   â””â”€â”€ zero_shot_mt5_20251117_110556.json
â”‚
â””â”€â”€ evaluations/
    â”œâ”€â”€ mbert_squad_dev_20251116_182000.json
    â”œâ”€â”€ mt5_squad_dev_20251117_110600.json
    â”œâ”€â”€ mbert_xquad_es_20251116_182100.json
    â””â”€â”€ mt5_xquad_es_20251117_110700.json
```

Each JSON file contains:
- Model configuration
- Training parameters
- Evaluation metrics (EM, F1, BLEU, ROUGE)
- Language-specific performance
- Statistical analysis

---

## ğŸ¯ Decision Tree: What Should I Do?

```
Do you want to...
â”‚
â”œâ”€â–º See quick results?
â”‚   â””â”€â–º Use: train_mt5_comparison.sh (30 min)
â”‚
â”œâ”€â–º Train a production model?
â”‚   â””â”€â–º Use: train_zero_shot.py with full data (2-4 hours)
â”‚
â”œâ”€â–º Compare mBERT vs mT5?
â”‚   â””â”€â–º Train both, then use: compare_models.py
â”‚
â”œâ”€â–º Test cross-lingual performance?
â”‚   â””â”€â–º Train zero-shot, then evaluate on XQuAD/MLQA
â”‚
â”œâ”€â–º Use the system interactively?
â”‚   â””â”€â–º Start API + Dashboard
â”‚
â””â”€â–º Understand the data?
    â””â”€â–º Use: notebooks/01_data_exploration.ipynb
```

---

## ğŸ’¡ Pro Tips

1. **Start Small**: Use quick training script first to verify everything works
2. **Check Logs**: Training logs in `logs/` show progress and errors
3. **Save Experiments**: All results are automatically tracked
4. **Use Notebooks**: Jupyter notebooks provide interactive exploration
5. **Monitor Memory**: mT5 uses more memory than mBERT
6. **Language Pairs**: Test both same-language and cross-lingual scenarios

---

This workflow diagram should help you understand how all the pieces fit together! ğŸš€

